# Lora and MTL-Lora a new frontier of Multi-task  fine-tuning for LM
ğŸš€ Medical Chatbot Fine-Tuning with LoRA & MTL-LoRA ğŸ¥ğŸ’¡

ğŸ” Overview

This project pushes the boundaries of medical AI by fine-tuning the LLaMA 1B model using LoRA and our custom-built Multi-Task LoRA (MTL-LoRA) framework, designed from scratch in PyTorch.

ğŸ“š Phase 1: Fine-Tuning with PubMedQA

We enhance the modelâ€™s medical expertise with PubMedQA, leveraging:

âš¡ LoRA: Efficient low-rank adaptation for faster, lightweight fine-tuning.

ğŸ› ï¸ Traditional Fine-Tuning: Updating only the last layer for controlled training.

ğŸ“Š How We Evaluate

We compare three configurations: Base Model, LoRA-Tuned Model, and Traditionally Fine-Tuned Model using:

ğŸ¯ Perplexity

ğŸ† BLEU Score

ğŸ“ˆ ROUGE Score

ğŸ¤– Phase 2: Custom Multi-Task LoRA (MTL-LoRA)

We built MTL-LoRA from scratch in PyTorch, allowing efficient multi-task learning across various medical NLP tasks in a single training pipeline. Inspired by cutting-edge research (arXiv 2410.09437), this approach ensures:

ğŸš€ Seamless multi-task adaptation without retraining per task.

ğŸ”¬ Enhanced generalization across diverse medical datasets.

ğŸ’° Reduced computational cost compared to full fine-tuning.

ğŸŒ Join the Future of Medical AI!

Contribute, experiment, and push the boundaries of whatâ€™s possible in AI-driven healthcare! ğŸ¥ğŸ’™

